{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80980a28",
   "metadata": {},
   "source": [
    "Langchain Reference: https://langchain-ai.github.io/langgraph/tutorials/rag/langgraph_crag/#llms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f97de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.messages import AnyMessage, SystemMessage\n",
    "from langchain.messages import ToolMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from typing import Literal\n",
    "from pydantic import BaseModel, Field\n",
    "from typing_extensions import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_community.vectorstores import FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8ba52e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "25ffbae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store_path = \"vectorstores/faiss_store\"\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "db = FAISS.load_local(\n",
    "    vector_store_path,\n",
    "    embeddings,\n",
    "    allow_dangerous_deserialization=True\n",
    ")\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}      \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e234907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binary_score='yes'\n"
     ]
    }
   ],
   "source": [
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check on retrieved documents.\"\"\"\n",
    "\n",
    "    binary_score: str = Field(\n",
    "        description=\"Documents are relevant to the question, 'yes' or 'no'\"\n",
    "    )\n",
    "\n",
    "# LLM with function call\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "structured_llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You are a grader assessing relevance of a retrieved document to a user question. \\n \n",
    "    If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant. \\n\n",
    "    Give a binary score 'yes' or 'no' score to indicate whether the document is relevant to the question.\"\"\"\n",
    "grade_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"Retrieved document: \\n\\n {document} \\n\\n User question: {question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | structured_llm_grader\n",
    "question = \"agent memory\"\n",
    "docs = retriever.invoke(question)\n",
    "doc_txt = docs[1].page_content\n",
    "print(retrieval_grader.invoke({\"question\": question, \"document\": doc_txt})) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "07e8eb53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In a LLM-powered autonomous agent system, memory is categorized into two types:\n",
      "\n",
      "1. **Short-term memory**: This involves in-context learning, where the model utilizes its immediate context to learn and adapt.\n",
      "\n",
      "2. **Long-term memory**: This allows the agent to retain and recall information over extended periods, often by using an external vector store for fast retrieval.\n"
     ]
    }
   ],
   "source": [
    "### Generate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Prompt\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a smart and helpful assistant designed to answer user questions.\n",
    "\n",
    "Instructions:\n",
    "1. If the user asks a general or social question (e.g., greetings, current date/time, small talk, simple facts), respond naturally using your own knowledge.\n",
    "2. If the question is factual or complex (e.g., about specific topics, facts, data, analysis), use ONLY the information from the provided context.\n",
    "3. DO NOT use your own knowledge to answer detailed or technical questions unless they are general facts.\n",
    "4. If the answer to a complex or factual question is NOT in the context, say:\n",
    "   \"I do not know based on the provided context.\"\n",
    "5. Be concise and factual. Avoid overexplaining unless it's directly supported by the context.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "# Chain\n",
    "rag_chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# Run\n",
    "generation = rag_chain.invoke({\"context\": docs, \"question\": question})\n",
    "print(generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a01ea",
   "metadata": {},
   "source": [
    "Question rewriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b64b821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is agent memory and how does it function in artificial intelligence?'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Question Re-writer\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Prompt\n",
    "system = \"\"\"You a question re-writer that converts an input question to a better version that is optimized \\n \n",
    "     for web search. Look at the input and try to reason about the underlying semantic intent / meaning.\"\"\"\n",
    "re_write_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\n",
    "            \"human\",\n",
    "            \"Here is the initial question: \\n\\n {question} \\n Formulate an improved question.\",\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "question_rewriter = re_write_prompt | llm | StrOutputParser()\n",
    "question_rewriter.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f76598",
   "metadata": {},
   "source": [
    "Web search tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "362d7881",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2 days ago - Barack Hussein Obama II (born August 4, 1961) is an American politician who served as the 44th president of the United States from 2009 to 2017. A member of the Democratic Party, he was the first African American president. Obama previously served as a U.S. senator representing Illinois from ... 3 weeks ago - Barack Hussein Obama II (/bəˈrɑːk huːˈseɪn oʊˈbɑːmə/ ; born August 4, 1961) is an American politician and attorney. He was the 44th president of the United States from 2009 to 2017. He was the first African-American president in U.S. history. A member of the Democratic Party, he ... November 25, 2025 - Obama is a surname. It most commonly refers to Barack Obama (born 1961), the 44th president of the United States. Obama is a common Fang masculine name in western Central Africa. 2 weeks ago - Barack Hussein Obama Sr. (/ˈbærək huːˈseɪn oʊˈbɑːmə/; born Baraka Obama, 18 June 1934 – 24 November 1982) was a Kenyan senior governmental economist and the father of Barack Obama, the 44th president of the United States. 3 weeks ago - The selection was slow because Malia is allergic to animal dander; the president subsequently said that the choice had been narrowed down to either a labradoodle or a Portuguese Water Dog, and that they hoped to find a shelter animal. On April 12, 2009, it was reported that the Obamas had adopted a six-month-old Portuguese Water Dog given to them as a gift by Senator Ted Kennedy; Malia and Sasha named the dog Bo. The White House referred to Bo as the First Dog.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchRun\n",
    "\n",
    "web_search_tool = DuckDuckGoSearchRun()\n",
    "\n",
    "web_search_tool.invoke(\"Obama's first name?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5484cdf",
   "metadata": {},
   "source": [
    "Define state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32e07d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "\n",
    "class GraphState(TypedDict):\n",
    "    \"\"\"\n",
    "    Represents the state of our graph.\n",
    "\n",
    "    Attributes:\n",
    "        question: question\n",
    "        generation: LLM generation\n",
    "        web_search: whether to add search\n",
    "        documents: list of documents\n",
    "    \"\"\"\n",
    "\n",
    "    question: str\n",
    "    generation: str\n",
    "    web_search: str\n",
    "    documents: List[str]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d55cfd2",
   "metadata": {},
   "source": [
    "Retrieve Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f221ac32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state):\n",
    "    \"\"\"\n",
    "    Retrieve documents\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): New key added to state, documents, that contains retrieved documents\n",
    "    \"\"\"\n",
    "    print(\"---RETRIEVE---\")\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    # Retrieval\n",
    "    documents = retriever.invoke(question)\n",
    "    return {\"documents\": documents}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c23843",
   "metadata": {},
   "source": [
    "Answer node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "575f954b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(state):\n",
    "    \"\"\"\n",
    "    Generate answer based on whether context documents are available.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state.\n",
    "\n",
    "    Returns:\n",
    "        dict: Updated state with 'generation' key containing the LLM response.\n",
    "    \"\"\"\n",
    "    print(\"---GENERATE---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state.get(\"documents\", None)\n",
    "\n",
    "    if documents:\n",
    "        # Use RAG generation with retrieved documents\n",
    "        generation = rag_chain.invoke({\"context\": documents, \"question\": question})\n",
    "    else:\n",
    "        # Fallback: General generation without retrieval context\n",
    "        general_prompt = ChatPromptTemplate.from_template(\n",
    "            \"\"\"\n",
    "You are a helpful and concise assistant.\n",
    "\n",
    "Answer the following question using general knowledge. Be accurate, polite, and to the point.\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "        )\n",
    "        general_chain = general_prompt | llm | StrOutputParser()\n",
    "        generation = general_chain.invoke({\"question\": question})\n",
    "\n",
    "    return {\n",
    "        \"documents\": documents,\n",
    "        \"question\": question,\n",
    "        \"generation\": generation\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95553850",
   "metadata": {},
   "source": [
    "Web search node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8e8214a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "def web_search(state):\n",
    "    \"\"\"\n",
    "    Web search based on the re-phrased question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with appended web results\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---WEB SEARCH---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Web search\n",
    "    raw_text = web_search_tool.invoke({\"query\": question})\n",
    "    doc = Document(\n",
    "    page_content=raw_text,\n",
    "    metadata={\"source\": \"duckduckgo\", \"type\": \"web\"}\n",
    ")\n",
    "    documents.append(doc)\n",
    "\n",
    "    return {\"documents\": documents, \"question\": question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f42f0a50",
   "metadata": {},
   "source": [
    "Define grader node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b9d8bd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grade_documents(state):\n",
    "    \"\"\"\n",
    "    Determines whether the retrieved documents are relevant to the question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates documents key with only filtered relevant documents\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---CHECK DOCUMENT RELEVANCE TO QUESTION---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Score each doc\n",
    "    filtered_docs = []\n",
    "    web_search = \"No\"\n",
    "    for d in documents:\n",
    "        score = retrieval_grader.invoke(\n",
    "            {\"question\": question, \"document\": d.page_content}\n",
    "        )\n",
    "        grade = score.binary_score\n",
    "        if grade == \"yes\":\n",
    "            print(\"---GRADE: DOCUMENT RELEVANT---\")\n",
    "            filtered_docs.append(d)\n",
    "        else:\n",
    "            print(\"---GRADE: DOCUMENT NOT RELEVANT---\")\n",
    "            web_search = \"Yes\"\n",
    "            continue\n",
    "    return {\"documents\": filtered_docs, \"web_search\": web_search}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbcf7ba4",
   "metadata": {},
   "source": [
    "Define query transforming node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e2984422",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def transform_query(state):\n",
    "    \"\"\"\n",
    "    Transform the query to produce a better question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        state (dict): Updates question key with a re-phrased question\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---TRANSFORM QUERY---\")\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "\n",
    "    # Re-write question\n",
    "    better_question = question_rewriter.invoke({\"question\": question})\n",
    "    return {\"documents\": documents, \"question\": better_question}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb4eb8e",
   "metadata": {},
   "source": [
    "Logic decide retrieve or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01139130",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_retrieve(state):\n",
    "    \"\"\"\n",
    "    Classify whether the question requires retrieval.\n",
    "    Returns:\n",
    "        state with key: \"retrieve\" = \"retrieve\" | \"no retrieve\"\n",
    "    \"\"\"\n",
    "    question = state[\"question\"]\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "You are a retrieval classifier.\n",
    "\n",
    "Task:\n",
    "Decide whether the following question requires retrieving information from external documents (e.g., a vector database or knowledge base).\n",
    "\n",
    "Guidelines:\n",
    "- Respond with exactly one word: \"retrieve\" or \"no retrieve\"\n",
    "- Use \"retrieve\" if the question asks for:\n",
    "  - Specific facts, data, or documents\n",
    "  - Names, statistics, figures, or dates that are not general/common knowledge\n",
    "  - Up-to-date or domain-specific knowledge (e.g., medical, legal, technical)\n",
    "- Use \"no retrieve\" if the question:\n",
    "  - Can be answered with general reasoning or common knowledge\n",
    "  - Involves small talk, greetings, or simple questions like the current date, weather, or definitions\n",
    "\n",
    "Only output one of the following:\n",
    "- retrieve\n",
    "- no retrieve\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "    decision = chain.invoke({\"question\": question})\n",
    "\n",
    "    return {\n",
    "        \"retrieve\": decision\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "758c867f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_retrieve(state):\n",
    "    \"\"\"\n",
    "    Decide whether to retrieve based on human question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---DECIDE TO RETRIEVE---\")\n",
    "    decide = state[\"retrieve\"]\n",
    "\n",
    "    if decide == \"retrieve\":\n",
    "        print(\"Decision: NEED RETRIEVAL\")\n",
    "        return \"retrieve\"\n",
    "    else:\n",
    "        print(\"Decision: NO RETRIEVAL NEEDED\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18924cf9",
   "metadata": {},
   "source": [
    "Logic to end node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77943a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decide_to_generate(state):\n",
    "    \"\"\"\n",
    "    Determines whether to generate an answer, or re-generate a question.\n",
    "\n",
    "    Args:\n",
    "        state (dict): The current graph state\n",
    "\n",
    "    Returns:\n",
    "        str: Binary decision for next node to call\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"---ASSESS GRADED DOCUMENTS---\")\n",
    "    state[\"question\"]\n",
    "    web_search = state[\"web_search\"]\n",
    "    state[\"documents\"]\n",
    "\n",
    "    if web_search == \"Yes\":\n",
    "        # All documents have been filtered check_relevance\n",
    "        # We will re-generate a new query\n",
    "        print(\n",
    "            \"---DECISION: ALL DOCUMENTS ARE NOT RELEVANT TO QUESTION, TRANSFORM QUERY---\"\n",
    "        )\n",
    "        return \"transform_query\"\n",
    "    else:\n",
    "        # We have relevant documents, so generate answer\n",
    "        print(\"---DECISION: GENERATE---\")\n",
    "        return \"generate\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a767f5e9",
   "metadata": {},
   "source": [
    "Graph building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd14763d",
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Define the nodes\n",
    "workflow.add_node(\"check_retrieve\", check_retrieve)  # check retrieve\n",
    "workflow.add_node(\"retrieve\", retrieve)  # retrieve\n",
    "workflow.add_node(\"grade_documents\", grade_documents)  # grade documents\n",
    "workflow.add_node(\"generate\", generate)  # generate\n",
    "workflow.add_node(\"transform_query\", transform_query)  # transform_query\n",
    "workflow.add_node(\"web_search_node\", web_search)  # web search\n",
    "\n",
    "# Build graph\n",
    "workflow.add_edge(START, \"check_retrieve\")\n",
    "# workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"check_retrieve\",\n",
    "    decide_to_retrieve,\n",
    "    {\n",
    "        \"retrieve\": \"retrieve\",\n",
    "        \"generate\": \"generate\",\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"transform_query\": \"transform_query\",\n",
    "        \"generate\": \"generate\",\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"transform_query\", \"web_search_node\")\n",
    "workflow.add_edge(\"web_search_node\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "app = workflow.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41f92916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---DECIDE TO RETRIEVE---\n",
      "Decision: NEED RETRIEVAL\n",
      "\"Node 'check_retrieve':\"\n",
      "'\\n---\\n'\n",
      "---RETRIEVE---\n",
      "\"Node 'retrieve':\"\n",
      "'\\n---\\n'\n",
      "---CHECK DOCUMENT RELEVANCE TO QUESTION---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---GRADE: DOCUMENT RELEVANT---\n",
      "---ASSESS GRADED DOCUMENTS---\n",
      "---DECISION: GENERATE---\n",
      "\"Node 'grade_documents':\"\n",
      "'\\n---\\n'\n",
      "---GENERATE---\n",
      "\"Node 'generate':\"\n",
      "'\\n---\\n'\n",
      "('The basic prompt engineering techniques mentioned in the context are:\\n'\n",
      " '\\n'\n",
      " '1. **Zero-Shot Learning**: This involves feeding the task text to the model '\n",
      " 'and asking for results without any prior examples.\\n'\n",
      " '\\n'\n",
      " '2. **Few-Shot Learning**: This presents a set of high-quality '\n",
      " 'demonstrations, each consisting of both input and desired output, to help '\n",
      " 'the model understand human intention and criteria for the task.')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "# Run\n",
    "inputs = {\"question\": \"types of basic prompt engineering techniques\"}\n",
    "for output in app.stream(inputs):\n",
    "    for key, value in output.items():\n",
    "        # Node\n",
    "        pprint(f\"Node '{key}':\")\n",
    "        # Optional: print full state at each node\n",
    "        # pprint.pprint(value[\"keys\"], indent=2, width=80, depth=None)\n",
    "    pprint(\"\\n---\\n\")\n",
    "\n",
    "# Final generation\n",
    "pprint(value[\"generation\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
